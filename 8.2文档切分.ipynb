{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文档切分\n",
    "***\n",
    "- 按照长度切分\n",
    "- 按照文本架构进行切分（句子、段落）\n",
    "- 按照文档格式切分\n",
    "- 基于语义进行切分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 长度切分\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path='deepseek.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(file_path)\n",
    "pages = []\n",
    "async for page in loader.alazy_load():\n",
    "    pages.append(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2. 算⼒需求分析\\n模型 参数规\\n模\\n计算精\\n度\\n最低显存需\\n求 最低算⼒需求\\nDeepSeek-R1 (671B)671B FP8 ≥890GB 2*XE9680（16*H20\\nGPU）\\nDeepSeek-R1-Distill-\\n70B 70B BF16 ≥180GB 4*L20 或 2*H20 GPU\\n三、国产芯⽚与硬件适配⽅案\\n1. 国内⽣态合作伙伴动态\\n企业 适配内容 性能对标（vs\\nNVIDIA）\\n华为昇\\n腾\\n昇腾910B原⽣⽀持R1全系列，提供端到端推理优化\\n⽅案 等效A100（FP16）\\n沐曦\\nGPU\\nMXN系列⽀持70B模型BF16推理，显存利⽤率提升\\n30% 等效RTX 3090\\n海光\\nDCU 适配V3/R1模型，性能对标NVIDIA A100 等效A100（BF16）\\n2. 国产硬件推荐配置\\n模型参数 推荐⽅案 适⽤场景\\n1.5B 太初T100加速卡 个⼈开发者原型验证\\n14B 昆仑芯K200集群 企业级复杂任务推理\\n32B 壁彻算⼒平台+昇腾910B集群 科研计算与多模态处理\\n四、云端部署替代⽅案\\n1. 国内云服务商推荐\\n平台 核⼼优势 适⽤场景']\n",
      "[Document(metadata={}, page_content='硅基流动 官⽅推荐API，低延迟，⽀持多模态模型 企业级⾼并发推理\\n腾讯云 ⼀键部署+限时免费体验，⽀持VPC私有化 中⼩规模模型快速上线\\nPPIO派欧云 价格仅为OpenAI 1/20，注册赠5000万tokens 低成本尝鲜与测试\\n2. 国际接⼊渠道（需魔法或外企上⽹环境\\n!\\n）\\n英伟达NIM：企业级GPU集群部署（链接）\\nGroq：超低延迟推理（链接）\\n五、完整671B MoE模型部署（Ollama+Unsloth）\\n1. 量化⽅案与模型选择\\n量化版本 ⽂件体\\n积\\n最低内存+显存需\\n求 适⽤场景\\nDeepSeek-R1-UD-\\nIQ1_M 158 GB ≥200 GB 消费级硬件（如Mac\\nStudio）\\nDeepSeek-R1-Q4_K_M 404 GB ≥500 GB ⾼性能服务器/云GPU\\n下载地址：\\nHuggingFace模型库\\nUnsloth AI官⽅说明\\n2. 硬件配置建议\\n硬件类型 推荐配置 性能表现（短⽂本⽣成）\\n消费级设备 Mac Studio（192GB统⼀内存） 10+ token/秒\\n⾼性能服务器4×RTX 4090（96GB显存+384GB内存） 7-8 token/秒（混合推理）\\n3. 部署步骤（Linux示例）\\n1. 安装依赖⼯具：\\n# 安装llama.cpp（⽤于合并分⽚⽂件）\\n/bin/bash -c \"$(curl -fsSL \\nhttps://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\\nbrew install llama.cpp'), Document(metadata={}, page_content='2. 下载并合并模型分⽚：\\n3. 安装Ollama：\\n4. 创建Modelfile：\\n5. 运⾏模型：\\n4. 性能调优与测试\\nGPU利⽤率低：升级⾼带宽内存（如DDR5 5600+）。\\n扩展交换空间：\\n六、注意事项与⻛险提示\\n1. 成本警示：\\n70B模型：需3张以上80G显存显卡（如RTX A6000），单卡⽤户不可⾏。\\n671B模型：需8xH100集群，仅限超算中⼼部署。\\n2. 替代⽅案：\\n个⼈⽤户推荐使⽤云端API（如硅基流动），免运维且合规。\\n3. 国产硬件兼容性：需使⽤定制版框架（如昇腾CANN、沐曦MXMLLM）。\\nllama-gguf-split --merge DeepSeek-R1-UD-IQ1_M-00001-of-00004.gguf \\nDeepSeek-R1-UD-IQ1_S.gguf\\ncurl -fsSL https://ollama.com/install.sh | sh\\nFROM /path/to/DeepSeek-R1-UD-IQ1_M.gguf  \\nPARAMETER num_gpu 28  # 每块RTX 4090加载7层（共4卡）  \\nPARAMETER num_ctx 2048  \\nPARAMETER temperature 0.6  \\nTEMPLATE \"<｜end▁of▁thinking｜>{{ .Prompt }}<｜end▁of▁thinking｜>\"\\nollama create DeepSeek-R1-UD-IQ1_M -f DeepSeekQ1_Modelfile\\nollama run DeepSeek-R1-UD-IQ1_M --verbose\\nsudo fallocate -l 100G /swapfile\\nsudo chmod 600 /swapfile\\nsudo mkswap /swapfile\\nsudo swapon /swapfile')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    encoding_name=\"cl100k_base\", chunk_size=50, chunk_overlap=10\n",
    ")\n",
    "texts = text_splitter.split_text(pages[1].page_content)\n",
    "print(texts)\n",
    "docs = text_splitter.create_documents([pages[2].page_content,pages[3].page_content])\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于文本架构\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2. 算⼒需求分析\\n模型 参数规\\n模\\n计算精\\n度\\n最低显存需\\n求 最低算⼒需求', 'DeepSeek-R1 (671B)671B FP8 ≥890GB 2*XE9680（16*H20', 'GPU）\\nDeepSeek-R1-Distill-', '70B 70B BF16 ≥180GB 4*L20 或 2*H20 GPU', '三、国产芯⽚与硬件适配⽅案\\n1. 国内⽣态合作伙伴动态\\n企业 适配内容 性能对标（vs', 'NVIDIA）\\n华为昇\\n腾\\n昇腾910B原⽣⽀持R1全系列，提供端到端推理优化', '⽅案 等效A100（FP16）\\n沐曦\\nGPU\\nMXN系列⽀持70B模型BF16推理，显存利⽤率提升', '30% 等效RTX 3090\\n海光', 'DCU 适配V3/R1模型，性能对标NVIDIA A100 等效A100（BF16）', '2. 国产硬件推荐配置\\n模型参数 推荐⽅案 适⽤场景', '1.5B 太初T100加速卡 个⼈开发者原型验证\\n14B 昆仑芯K200集群 企业级复杂任务推理', '32B 壁彻算⼒平台+昇腾910B集群 科研计算与多模态处理\\n四、云端部署替代⽅案', '1. 国内云服务商推荐\\n平台 核⼼优势 适⽤场景']\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=50, chunk_overlap=0)\n",
    "texts = text_splitter.split_text(pages[1].page_content)\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基于文档架构\n",
    "****\n",
    "- markdown 根据标题拆分（例如，#、##、###）\n",
    "- JSON：按对象或数组元素拆分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -qU langchain-text-splitters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于markdown格式进行切分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Header 1': 'Foo', 'Header 2': 'Bar'}, page_content='Hi this is Jim  \\nHi this is Joe'),\n",
       " Document(metadata={'Header 1': 'Foo', 'Header 2': 'Bar', 'Header 3': 'Boo'}, page_content='Hi this is Lance'),\n",
       " Document(metadata={'Header 1': 'Foo', 'Header 2': 'Baz'}, page_content='Hi this is Molly')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "markdown_document = \"# Foo\\n\\n    ## Bar\\n\\nHi this is Jim\\n\\nHi this is Joe\\n\\n ### Boo \\n\\n Hi this is Lance \\n\\n ## Baz\\n\\n Hi this is Molly\"\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on)\n",
    "md_header_splits = markdown_splitter.split_text(markdown_document)\n",
    "md_header_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于JSON格式进行切分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "json_data = requests.get(\"https://api.smith.langchain.com/openapi.json\").json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'openapi': '3.1.0', 'info': {'title': 'LangSmith', 'version': '0.1.0'}, 'paths': {'/api/v1/sessions/{session_id}': {'get': {'tags': ['tracer-sessions'], 'summary': 'Read Tracer Session', 'description': 'Get a specific session.'}}}}\n",
      "{'paths': {'/api/v1/sessions/{session_id}': {'get': {'operationId': 'read_tracer_session_api_v1_sessions__session_id__get', 'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}}}}\n",
      "{'paths': {'/api/v1/sessions/{session_id}': {'get': {'parameters': [{'name': 'session_id', 'in': 'path', 'required': True, 'schema': {'type': 'string', 'format': 'uuid', 'title': 'Session Id'}}, {'name': 'include_stats', 'in': 'query', 'required': False, 'schema': {'type': 'boolean', 'default': False, 'title': 'Include Stats'}}, {'name': 'accept', 'in': 'header', 'required': False, 'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Accept'}}]}}}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveJsonSplitter\n",
    "\n",
    "splitter = RecursiveJsonSplitter(max_chunk_size=300)\n",
    "json_chunks = splitter.split_json(json_data=json_data)\n",
    "\n",
    "for chunk in json_chunks[:3]:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='{\"openapi\": \"3.1.0\", \"info\": {\"title\": \"LangSmith\", \"version\": \"0.1.0\"}, \"paths\": {\"/api/v1/sessions/{session_id}\": {\"get\": {\"tags\": [\"tracer-sessions\"], \"summary\": \"Read Tracer Session\", \"description\": \"Get a specific session.\"}}}}'\n",
      "page_content='{\"paths\": {\"/api/v1/sessions/{session_id}\": {\"get\": {\"operationId\": \"read_tracer_session_api_v1_sessions__session_id__get\", \"security\": [{\"API Key\": []}, {\"Tenant ID\": []}, {\"Bearer Auth\": []}]}}}}'\n",
      "page_content='{\"paths\": {\"/api/v1/sessions/{session_id}\": {\"get\": {\"parameters\": [{\"name\": \"session_id\", \"in\": \"path\", \"required\": true, \"schema\": {\"type\": \"string\", \"format\": \"uuid\", \"title\": \"Session Id\"}}, {\"name\": \"include_stats\", \"in\": \"query\", \"required\": false, \"schema\": {\"type\": \"boolean\", \"default\": false, \"title\": \"Include Stats\"}}, {\"name\": \"accept\", \"in\": \"header\", \"required\": false, \"schema\": {\"anyOf\": [{\"type\": \"string\"}, {\"type\": \"null\"}], \"title\": \"Accept\"}}]}}}}'\n"
     ]
    }
   ],
   "source": [
    "# 生成langchain Document\n",
    "docs = splitter.create_documents(texts=[json_data])\n",
    "\n",
    "for doc in docs[:3]:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基于语义切分\n",
    "*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --quiet langchain_experimental langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"meow.txt\") as f:\n",
    "    meow = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "# 使用OpenAIEmbeddings进行向量化\n",
    "text_splitter = SemanticChunker(OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meow meow🐱 \n",
      " meow meow🐱 \n",
      " meow😻😻\n"
     ]
    }
   ],
   "source": [
    "docs = text_splitter.create_documents([meow])\n",
    "print(docs[0].page_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
