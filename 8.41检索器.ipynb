{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 检索器\n",
    "***\n",
    "- 基本检索器设置\n",
    "- 词法搜索检索器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基本检索器设置\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "loader = TextLoader(\"test.txt\")\n",
    "\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实例化检索器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.invoke(\"deepseek是什么？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='e60ca19a-6b5f-4995-9729-2e2aa6c08782', metadata={'source': 'test.txt'}, page_content='Deepseek R1 是⽀持复杂推理、多模态处理、技术⽂档⽣成的⾼性能通⽤⼤语⾔模型。本⼿册\\n为技术团队提供完整的本地部署指南，涵盖硬件配置、国产芯⽚适配、量化⽅案、云端替代⽅\\n案及完整671B MoE模型的Ollama部署⽅法。模型 参数规\\n模\\n计算精\\n度\\n最低显存需\\n求 最低算⼒需求\\nDeepSeek-R1 (671B) 671B FP8 ≥890GB\\n2*XE9680（16*H20\\nGPU）\\nDeepSeek-R1-Distill-\\n70B\\n70B BF16 ≥180GB 4*L20 或 2*H20 GPU\\n三、国产芯⽚与硬件适配⽅案\\n1. 国内⽣态合作伙伴动态\\n企业 适配内容 性能对标（vs\\nNVIDIA）\\n华为昇\\n腾\\n昇腾910B原⽣⽀持R1全系列，提供端到端推理优化\\n⽅案 等效A100（FP16）\\n沐曦\\nGPU\\nMXN系列⽀持70B模型BF16推理，显存利⽤率提升\\n30% 等效RTX 3090\\n海光\\nDCU 适配V3/R1模型，性能对标NVIDIA A100 等效A100（BF16）\\n2. 国产硬件推荐配置\\n模型参数 推荐⽅案 适⽤场景\\n1.5B 太初T100加速卡 个⼈开发者原型验证\\n14B 昆仑芯K200集群 企业级复杂任务推理\\n32B 壁彻算⼒平台+昇腾910B集群 科研计算与多模态处理\\n12月26日，Deepseek发布了全新系列模型DeepSeek-v3，一夜之间霸榜开源模型，并在性能上和世界顶尖的闭源模型GPT-4o以及 Claude-3.5-Sonnet相提并论。\\n\\n该模型为MOE架构，大大降低了训练成本，据说训练成本仅600万美元，成本降低10倍，资源运用效率极高。有AI投资机构负责人直言，DeepSeek发布的53页的技术论文是黄金。\\n\\n那就先让我们看看论文是怎么说的吧，老规矩，先上资源地址：\\n\\nGithub: GitHub - deepseek-ai/DeepSeek-V3\\n\\n模型地址：https://huggingface.co/deepseek-ai\\n\\n论文地址：https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf\\n\\n以下为技术解读：'),\n",
       " Document(id='96a1208b-9158-49c1-bcea-8f84a5bd36f1', metadata={'source': 'test.txt'}, page_content='结论、局限性和未来方向\\nDeepSeek-V3 是一款性能强大且成本效益高的开源大型语言模型，它在推理和生成任务中都取得了显著的成果。DeepSeek-V3 的训练成本非常低，只需 2.788M H800 GPU 小时即可完成其全部训练，包括预训练、上下文长度扩展和后训练。\\n\\n尽管 DeepSeek-V3 在性能和效率方面取得了显著成果，但它仍然存在一些局限性，尤其是在部署方面。DeepSeek-V3 的推荐部署单元相对较大，这可能对小型团队构成负担。此外，尽管 DeepSeek-V3 的部署策略已经实现了比 DeepSeek-V2 高两倍的端到端生成速度，但仍然存在进一步提升的空间。\\n\\nDeepSeek-V3 开发了创新的负载平衡策略和训练目标，以实现高效训练。它还引入了 FP8 训练和一系列高效的工程优化措施，以进一步降低训练成本。\\nDeepSeek-V3 还在后训练阶段取得了成功，通过知识蒸馏和强化学习技术，显著提高了其在数学和代码基准测试中的性能。\\nDeepSeek-V3 在一系列基准测试中取得了最强大的性能，尤其是在数学、代码和长上下文理解任务上。\\nDeepSeek-V3 的局限性主要在于部署方面，包括较大的部署单元和潜在的性能提升空间。\\nDeepSeek-V3 采用了宪法 AI （constitutional AI） 方法，利用 DeepSeek-V3 自身的投票评估结果作为反馈来源，进一步提高了其在主观评估中的性能。\\nDeepSeek-V3 中的多 token 预测技术可以显著加速模型的解码速度，而额外的预测 token 的接受率在 85% 到 90% 之间，这表明其具有高度的可靠性。\\nDeepSeek 持续致力于开源模型的道路，并计划在未来进行以下方面的研究：'),\n",
       " Document(id='03134a40-6701-440d-aa93-5e3051f9b553', metadata={'source': 'test.txt'}, page_content='进一步改进模型架构，以提高训练和推理效率，并尝试突破 Transformer 架构的限制。\\n持续迭代训练数据的质量和数量，并探索其他训练信号来源，以推动数据扩展到更广泛的维度。\\n持续探索和迭代模型的深度思考能力，以增强其智能和问题解决能力，并扩展其推理长度和深度。\\n探索更全面和多维度的模型评估方法，以防止在研究过程中优化固定的一组基准测试，从而产生对模型能力的误导印象并影响我们的基础评估。\\nDeepSeek-V3 的发布标志着开源大型语言模型领域的一个重大里程碑，并为未来的研究和应用开辟了新的可能性。\\n\\n简单测试\\nDeepSeek-V3开源模型，我肯定是没有资源部署了，所以只能通过它的服务网站进行测试了。\\n\\n地址：DeepSeek\\n\\n\\n算一下星舰从地球到火星的飞行时间：\\n\\n\\n让它分析一下自己的技术文档：\\n\\n\\n最后让它比较了一下自己与GPT-4o-0513\\n\\n\\n... 略...\\n\\n\\n——完——\\n\\n@北方的郎 · 专注模型与代码\\n\\n概述\\n前置准备\\n1. 申请 DeepSeek API\\n2. 注册 Dify\\n集成步骤\\n1. 将 DeepSeek 接入至 Dify\\n2. 搭建 DeepSeek AI 应用\\n3. 为 AI 应用启用文本分析能力\\n4. 分享 AI 应用\\n阅读更多\\nEdit on GitHub\\n\\n阅读更多\\n应用案例\\nDeepSeek 与 Dify 集成指南：打造具备多轮思考的 AI 应用\\n概述\\nDeepSeek 作为具备多轮推理能力的开源大语言模型，以高性能、低成本、易部署的特性成为智能应用开发的理想基座。通过其 API 服务，开发者可快速调用 DeepSeek 的复杂逻辑推理与内容生成能力。在传统开发模式下，构建生产级 AI 应用往往需要独立完成模型适配、接口开发、交互设计等环节。\\n\\nDify 作为同样开源的生成式 AI 应用开发平台，能够帮助开发者基于 DeepSeek 大模型快速开发出更加智能的 AI 应用，你可以在 Dify 平台内获得以下开发体验：\\n\\n可视化构建 - 通过可视化编排界面，3 分钟搭建基于 DeepSeek R1 的 AI 应用\\n\\n知识库增强 - 关联内部文档，开启 RAG 能力并构建精准问答系统\\n\\n工作流扩展 - 提供多种第三方工具插件、可视化拖拽式编排应用功能节点，实现复杂业务逻辑'),\n",
       " Document(id='10981ea0-951a-4c20-bfff-6b264f196056', metadata={'source': 'test.txt'}, page_content='数据洞察力 - 内置总对话数、应用使用用户数等数据监控模块，支持与更加专业的监控平台集成 ...\\n\\n本文将详解 DeepSeek API 与 Dify 的集成步骤，助你快速实现两大核心场景：\\n\\n智能对话机器人开发 - 直接调用 DeepSeek R1 的思维链推理能力\\n\\n知识增强型应用构建 - 通过私有知识库实现精准信息检索与生成\\n\\n针对金融、法律等高合规需求场景，Dify 提供 本地私有化部署 DeepSeek + Dify，支持 DeepSeek 模型与 Dify 平台同步部署至内网\\n\\n通过 Dify × DeepSeek 的技术组合，开发者可跳过底层架构搭建，跃迁至场景化 AI 能力落地阶段，让大模型技术快速转化为业务生产力。\\n\\n前置准备\\n1. 申请 DeepSeek API\\n访问 DeepSeek API 开放平台，按照页面提示进行申请 API Key。\\n\\n若提示链接无法访问，你也可以考虑在本地部署 DeepSeek 模型。详细说明请参考 本地部署指南\\n\\n2. 注册 Dify\\nDify 是一个能够帮助你快速搭建生成式 AI 应用的平台，通过接入 DeepSeek API，你可以快速搭建出一个能够易于使用的 DeepSeek AI 应用。\\n\\n集成步骤\\n1. 将 DeepSeek 接入至 Dify\\n访问 Dify 平台，点击右上角头像 → 设置 → 模型供应商，找到 DeepSeek，将上文获取的 API Key 粘贴至其中。点击保存，校验通过后将出现成功提示。\\n\\n\\n2. 搭建 DeepSeek AI 应用\\n轻点 Dify 平台首页左侧的\"创建空白应用\"，选择\"聊天助手\"类型应用并进行简单的命名。\\n\\n\\n选择 deepseek-reasoner 模型\\n\\ndeepseek-reasoner 模型又称为 deepseek-r1 模型。\\n\\n\\n配置完成后即可在聊天框中进行互动。\\n\\n\\n3. 为 AI 应用启用文本分析能力\\nRAG（检索增强生成）是一种先进的信息处理技术，它通过检索相关知识，向 LLM 提供必要的上下文信息，融入 LLM 的内容生成过程，提升回答的准确性和专业度。当你上传内部文档或专业资料后，AI 能够基于这些知识提供更有针对性的解答。')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BM25也称为Okapi BM25，是信息检索系统中用来估计文档与给定搜索查询的相关性的排名函数。\n",
    "****\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade --quiet  rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "vretriever = BM25Retriever.from_texts([\"foo\", \"bar\", \"world\", \"hello\", \"foo bar\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用所创建的文档来创建一个新的检索器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "retriever = BM25Retriever.from_documents(\n",
    "    [\n",
    "        Document(page_content=\"foo\"),\n",
    "        Document(page_content=\"bar\"),\n",
    "        Document(page_content=\"world\"),\n",
    "        Document(page_content=\"hello\"),\n",
    "        Document(page_content=\"foo bar\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = retriever.invoke(\"foo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='foo'),\n",
       " Document(metadata={}, page_content='foo bar'),\n",
       " Document(metadata={}, page_content='hello'),\n",
       " Document(metadata={}, page_content='world')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用预处理器进行增强，在词语级别检索效果比较显著"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "punkt 是一个句子分割器模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/zhangyaohui/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='bar'),\n",
       " Document(metadata={}, page_content='foo bar')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "retriever = BM25Retriever.from_documents(\n",
    "    [\n",
    "        Document(page_content=\"foo\"),\n",
    "        Document(page_content=\"bar\"),\n",
    "        Document(page_content=\"world\"),\n",
    "        Document(page_content=\"hello\"),\n",
    "        Document(page_content=\"foo bar\"),\n",
    "    ],\n",
    "    k=2,\n",
    "    preprocess_func=word_tokenize,\n",
    ")\n",
    "\n",
    "result = retriever.invoke(\"bar\")\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
